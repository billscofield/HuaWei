# Token Bucket Filter(TBF)

[link](https://www.zhihu.com/question/21053403)

|                   
|   █ █ █ █ █ █ █ █ █---+           +---------- tokens replensished at rate
|                       |           |
|                       |           |
|                      \|/         \|/
|                   |       |   |       |<--+
|                   |       |   |Tokens |   |
|                   |       |   |   +   |   |
|                   | FIFO  |   |       |   |
|                   |       |   |   +   |   |   bucket size
|                   |       |   |       |   |   limits number
|                   |       |   |   +   |   |   of instantaneously
|                   |       |   |       |   |   available tokens
|                   |       |   |   +   |   |
|                   |       |   |       |   |
|                   |       |   |   +   | <-+
|                   |       |   |       |
|                       |           |
|                       |           |
|                       |           +-----------+
|                       |                       |
|                       |                      \|/
|                       |                       |
|                       |                      / \                  packets transmitted at rate
|                       |                     /   \         yes     
|                       +----------------->  /Token \ ----------> ▉ ▉ ▉ █ █ █ █ █ █ █ █
|                                            Available
|                                             \   /
|                                              \ /
|                                               |
|                                               |  no
|                                              \|/
|                                               |  wait until token(s) are available


[link](https://www.lartc.org/howto/lartc.qdisc.classless.html)

The Token Bucket Filter (TBF) is a simple qdisc that only passes packets
arriving at a rate which is not exceeding some administratively set rate, but
with the possibility to allow short bursts in excess of this rate.

TBF is very precise, network- and processor friendly. It should be your first
choice if you simply want to slow an interface down!

The TBF implementation consists of a buffer (bucket), constantly filled by some
virtual pieces of information called tokens, at a specific rate (token rate).
The most important parameter of the bucket is its size, that is the number of
tokens it can store.

Each arriving token collects one incoming data packet from the data queue and
is then deleted from the bucket. Associating this algorithm with the two flows
-- token and data, gives us three possible scenarios:

    1. The data arrives in TBF at a rate that's equal to the rate of incoming
       tokens.  In this case each incoming packet has its matching token and
       passes the queue without delay.

    2. The data arrives in TBF at a rate that's smaller than the token rate.
       Only a part of the tokens are deleted at output of each data packet
       that's sent out the queue, so the tokens accumulate, up to the bucket
       size. **The unused tokens can then be used to send data at a speed that's
       exceeding the standard token rate, in case short data bursts occur.**

    3. The data arrives in TBF at a rate bigger than the token rate. This means
       that the bucket will soon be devoid(缺乏，完全没有) of tokens, which
       causes the TBF to throttle(/ ˈθrɑːtl/节流阀) itself for a while. This is
       called an 'overlimit situation'. If packets keep coming in, packets will
       start to get dropped.

**The accumulation of tokens allows a short burst of overlimit data to be still
passed without loss, but any lasting overload will cause packets to be
constantly delayed, and then dropped.**

**Please note that in the actual implementation, tokens correspond (相一致，符合)
to bytes, not packets.**


## 9.2.2.1. Parameters & usage

Even though you will probably not need to change them, tbf has some knobs(/nɑː
b/球形把手；旋钮) available. First the parameters that are always available:

1. limit or latency

    Limit is the number of bytes that can be queued waiting for tokens to
    become available.

    You can also specify this the other way around by setting the latency(/ˈleɪ
    tənsi/等待时间) parameter, which specifies the maximum amount of time a
    packet can sit in the TBF.

    The latter calculation takes into account the size of the bucket, the rate
    and possibly the peakrate (if set).

2. burst/buffer/maxburst

    Size of the bucket, in bytes.

    This is the maximum amount of bytes that tokens can be available for
    instantaneously.

    In general, larger shaping rates require a larger buffer. For 10mbit/s on
    Intel, you need at least 10kbyte buffer if you want to reach your
    configured rate!

    If your buffer is too small, packets may be dropped because more tokens
    arrive per timer tick than fit in your bucket.

3. mpu

    A zero-sized packet does not use zero bandwidth. For ethernet, no packet
    uses less than 64 bytes. The Minimum Packet Unit determines the minimal
    token usage for a packet.

4. rate

    The speedknob. See remarks above about limits!

If the bucket contains tokens and is allowed to empty, by default it does so at
infinite speed. If this is unacceptable, use the following parameters:

1. peakrate

    If tokens are available, and packets arrive, they are sent out immediately
    by default, at 'lightspeed' so to speak. That may not be what you want,
    especially if you have a large bucket.

    The peakrate can be used to specify how quickly the bucket is allowed to be
    depleted(/dɪˈpliːt/耗尽). If doing everything by the book, this is achieved
    by releasing a packet, and then wait just long enough, and release the
    next. We calculated our waits so we send just at peakrate.

    However, due to the default 10ms timer resolution of Unix, with 10.000 bits
    average packets, we are limited to 1mbit/s of peakrate!

2. mtu/minburst

    The 1mbit/s peakrate is not very useful if your regular rate is more than
    that. A higher peakrate is possible by sending out more packets per
    timertick, which effectively means that we create a second bucket!

    This second bucket defaults to a single packet, which is not a bucket at
    all.

    To calculate the maximum possible peakrate, multiply the configured mtu by
    100 (or more correctly, HZ, which is 100 on Intel, 1024 on Alpha).



## 9.2.2.2. Sample configuration

A simple but *very* useful configuration is this:

    > tc qdisc add dev ppp0 root tbf rate 220kbit latency 50ms burst 1540

Ok, why is this useful? If you have a networking device with a large queue,
like a DSL modem or a cable modem, and you talk to it over a fast device, like
over an ethernet interface, you will find that uploading absolutely destroys
interactivity.

This is because uploading will fill the queue in the modem, which is probably
*huge* because this helps actually achieving good data throughput uploading.
But this is not what you want, you want to have the queue not too big so
interactivity remains and you can still do other stuff while sending data.

The line above slows down sending to a rate that does not lead to a queue in
the modem - the queue will be in Linux, where we can control it to a limited
size.

Change 220kbit to your uplink's *actual* speed, minus a few percent. If you
have a really fast modem, raise 'burst' a bit.

