# Hugging face

[官网](http://chemex.wrlca.com/)

Hugging face 起初是一家总部位于纽约的聊天机器人初创服务商，他们本来打算创业做聊
天机器人，然后在github上开源了一个Transformers库，虽然聊天机器人业务没搞起来，
但是他们的这个库在机器学习社区迅速大火起来。目前已经共享了超100,000个预训练模型，
10,000个数据集，变成了机器学习界的github。

其之所以能够获得如此巨大的成功，一方面是让我们这些甲方企业的小白，尤其是入门者
也能快速用得上科研大牛们训练出的超牛模型。另一方面是，这种特别开放的文化和态度，
以及利他利己的精神特别吸引人。huggingface上面很多业界大牛也在使用和提交新模型，
这样我们就是站在大牛们的肩膀上工作，而不是从头开始，当然我们也没有大牛那么多的
计算资源和数据集。

在国内huggingface也是应用非常广泛，一些开源框架本质上就是调用transfomer上的模型
进行微调（当然也有很多大牛在默默提供模型和数据集）。很多nlp工程师招聘的条目上也
明摆着要求熟悉huggingface transformer库的使用。简单介绍了他们多么牛逼之后，我们
看看huggingface怎么玩吧。因为他既提供了数据集，又提供了模型让你随便调用下载，因
此入门非常简单。你甚至不需要知道什么是GPT，BERT就可以用他的模型了（当然看看我写
的BERT简介还是十分有必要的）。下面初步介绍下huggingface里面都有什么，以及怎么调
用BERT模型做个简单的任务。

在这里主要有以下大家需要的资源。

- Datasets：数据集，以及数据集的下载地址
- Models：各个预训练模型
- course：免费的nlp课程，可惜都是英文的
- docs：文档

在NLP领域，在hugging face上面数据集和预训练模型的数量以英语为最为众多，远超其他
国家的总和。就预训练模型来说，排名第二的是汉语。就数据集来说，汉语远远少于英语，
也少于法，德，西班牙等语言，甚至少于阿拉伯语和波兰语。这严重跟我想象中的AI超级
大国及其不匹配。我想一方面因为数据集的积累都需要很多年，中文常用的（PKU，MSRA）
数据集都是十几年前留下的，而我们AI和经济的崛起也不过是最近十年的事情。另一方面，
数据集都是大价钱整理出来的，而且可以不断的利用他产生新的模型，这样的大杀器怎可
随意公布。发布预训练模型可以带来论文，数据集可啥也带不来，基本上中日韩等的数据
集明显偏少。

[参考](https://zhuanlan.zhihu.com/p/535100411)
